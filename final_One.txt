import os
import sys
import re
import html
import gc
import logging
from datetime import datetime
from functools import lru_cache
from multiprocessing import Pool, Manager
from threading import Lock as ThreadLock
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import psutil

import boto3
import pandas as pd
import numpy as np
import configparser
import zipfile
import io
from tqdm import tqdm
from tqdm.contrib.concurrent import thread_map
from jinja2 import Template

# Global lock for thread-safe operations
tqdm_lock = ThreadLock()
print_lock = ThreadLock()

'''-------------------------------------------------------------
Setup Logging
-------------------------------------------------------------'''
LOG_DIR = os.path.join(os.getcwd(), 'reports', 'log')
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE = os.path.join(LOG_DIR, f'log_{ts}.log')

os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

'''-------------------------------------------------------------
Configuration and Constants
-------------------------------------------------------------'''
pd.set_option('mode.chained_assignment', None)

config = configparser.ConfigParser()
try:
    config.read('config.ini')
except Exception as e:
    logging.error(f"Failed to read config.ini: {e}")
    raise

# [settings]
project_name = config.get('settings', 'project_name', fallback='CSV Comparison')
project_logo = config.get('settings', 'project_logo', fallback='logo.png')

# [report]
output_dir = config.get('report', 'output_dir', fallback='reports')
output_file = config.get('report', 'output_file', fallback='report.html')
download_local = config.getboolean('download', 'download_local', fallback=False)

# [keys]
csv_primary_keys = config.get('keys', 'primary_key_columns', fallback='')
csv_columns = config.get('keys', 'columns', fallback='')
csv_primary_keys = [col.strip() for col in csv_primary_keys.split(',') if col.strip()] or []
csv_columns = [col.strip() for col in csv_columns.split(',') if col.strip()] or None

# [aws]
bucket_name = config.get('aws', 'bucket_name', fallback='')
source_1_prefix = config.get('aws', 'source_1_prefix', fallback='')
source_2_prefix = config.get('aws', 'source_2_prefix', fallback='')

# [threading]
use_multithreading_reading = config.getboolean('threading', 'use_multithreading_reading', fallback=True)
use_multithreading_comparison = config.getboolean('threading', 'use_multithreading_comparision', fallback=True)
num_processes = config.getint('threading', 'num_processes', fallback=max(1, os.cpu_count() or 4))
comparison_batch_size = config.getint('threading', 'comparison_batch_size', fallback=50)
num_chunks = config.getint('threading', 'num_chunks', fallback=3)
show_row_progress = config.getboolean('threading', 'show_row_progress', fallback=True)

# [report_custom]
include_passed = config.getboolean('report_custom', 'include_passed', fallback=True)
include_missing_files = config.getboolean('report_custom', 'include_missing_files', fallback=True)
include_extra_files = config.getboolean('report_custom', 'include_extra_files', fallback=True)

# [global_col]
global_percentage = config.get('global_col', 'global_percentage', fallback='')
global_percentage = [col.strip() for col in global_percentage.split(',') if col.strip()] or []

'''-------------------------------------------------------------
Utility Functions
-------------------------------------------------------------'''
def normalize_value(val):
    if pd.isna(val) or val is None:
        return np.nan
    if isinstance(val, (int, float)):
        return float(val)
    if isinstance(val, str):
        val = val.strip()
        if val.lower() in ('null', 'none', 'nan', ''):
            return np.nan
        try:
            return float(val) if '.' in val else int(val)
        except ValueError:
            return val.lower()
    return val

def values_equal(val1, val2):
    val1 = normalize_value(val1)
    val2 = normalize_value(val2)
    return pd.isna(val1) and pd.isna(val2) or val1 == val2

def normalize_filename(filename):
    return re.sub(r'\d{8}_\d{4}', '', os.path.basename(filename))

def thread_safe_print(*args, **kwargs):
    with print_lock:
        logging.info(*args, **kwargs)

def log_resource_usage():
    process = psutil.Process()
    cpu_percent = psutil.cpu_percent(interval=0.1)
    memory_info = process.memory_info()
    logging.info(f"Resource Usage: CPU {cpu_percent}% | Memory {memory_info.rss / 1024**2:.2f} MB | PID {os.getpid()}")

'''-------------------------------------------------------------
S3 and File Handling Functions
-------------------------------------------------------------'''
def get_s3_client(profile_name='p3-dev'):
    try:
        session = boto3.session.Session(profile_name=profile_name)
        return session.client('s3', config=boto3.session.Config(max_pool_connections=50))
    except Exception as e:
        logging.error(f"Failed to create S3 client: {e}")
        raise

s3 = get_s3_client() if not download_local else None

def list_zip_files(prefix, download_local):
    if download_local:
        folder = os.path.join("downloads", prefix)
        try:
            if not os.path.exists(folder):
                raise FileNotFoundError(f"Local folder not found: {folder}")
            zip_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.zip')]
            logging.info(f"Found {len(zip_files)} ZIP files in {folder}")
            return zip_files
        except Exception as e:
            thread_safe_print(f"‚ùå {type(e).__name__}: {e}")
            logging.error(f"Error listing local ZIP files: {e}")
            return []
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        zip_files = [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith('.zip')]
        logging.info(f"Found {len(zip_files)} ZIP files in S3 bucket {bucket_name}/{prefix}")
        return zip_files
    except Exception as e:
        thread_safe_print(f"‚ùå {type(e).__name__}: {e}")
        logging.error(f"Error listing S3 ZIP files: {e}")
        return []

def list_csvs_in_zip(zip_key, download_local):
    try:
        if download_local:
            with zipfile.ZipFile(zip_key, 'r') as z:
                return [f for f in z.namelist() if f.endswith('.csv')]
        zip_obj = s3.get_object(Bucket=bucket_name, Key=zip_key)
        zip_data = zipfile.ZipFile(io.BytesIO(zip_obj['Body'].read()))
        return [f for f in zip_data.namelist() if f.endswith('.csv')]
    except Exception as e:
        thread_safe_print(f"‚ùå Error listing CSVs in {zip_key}: {e}")
        logging.error(f"Error listing CSVs in {zip_key}: {e}")
        return []

def read_csv_from_zip(zip_key, csv_filename, download_local, chunksize=10000):
    try:
        if download_local:
            with zipfile.ZipFile(zip_key, 'r') as z:
                with z.open(csv_filename) as f:
                    return pd.concat([chunk for chunk in pd.read_csv(f, low_memory=False, chunksize=chunksize)])
        zip_obj = s3.get_object(Bucket=bucket_name, Key=zip_key)
        zip_data = zipfile.ZipFile(io.BytesIO(zip_obj['Body'].read()))
        with zip_data.open(csv_filename) as f:
            return pd.concat([chunk for chunk in pd.read_csv(f, low_memory=False, chunksize=chunksize)])
    except Exception as e:
        thread_safe_print(f"‚ùå Error reading CSV {csv_filename} from {zip_key}: {e}")
        logging.error(f"Error reading CSV {csv_filename} from {zip_key}: {e}")
        return None

def read_csv_chunk(zip_csv_pairs, source_name, download_local, use_multithreading=True):
    chunk_csvs = {}
    store_lock = ThreadLock()

    zip_to_csvs = {}
    for zip_key, csv_name in zip_csv_pairs:
        zip_to_csvs.setdefault(zip_key, []).append(csv_name)

    def read_zip_and_store(zip_key, csv_filenames):
        try:
            if download_local:
                with zipfile.ZipFile(zip_key, 'r') as z:
                    for csv_name in csv_filenames:
                        with z.open(csv_name) as f:
                            df = pd.read_csv(f, low_memory=False)
                            with store_lock:
                                chunk_csvs[csv_name] = df
            else:
                zip_obj = s3.get_object(Bucket=bucket_name, Key=zip_key)
                zip_data = zipfile.ZipFile(io.BytesIO(zip_obj['Body'].read()))
                for csv_name in csv_filenames:
                    with zip_data.open(csv_name) as f:
                        df = pd.read_csv(f, low_memory=False)
                        with store_lock:
                            chunk_csvs[csv_name] = df
        except Exception as e:
            thread_safe_print(f"‚ùå Failed to read {zip_key}: {e}")
            logging.error(f"Failed to read {zip_key}: {e}")

    if use_multithreading:
        with tqdm(total=len(zip_to_csvs), desc=f"Reading CSVs from {source_name}", unit="zip", dynamic_ncols=True):
            thread_map(
                lambda x: read_zip_and_store(x[0], x[1]),
                zip_to_csvs.items(),
                max_workers=8
            )
    else:
        for zip_key, csv_filenames in tqdm(
            zip_to_csvs.items(),
            desc=f"Reading CSVs from {source_name}",
            unit="zip",
            dynamic_ncols=True
        ):
            read_zip_and_store(zip_key, csv_filenames)

    return chunk_csvs

def read_all_csvs_by_source(zip_keys, source_name, download_local):
    zip_to_csvs = {}
    for zip_key in tqdm(zip_keys, desc=f"Listing CSVs from {source_name}", unit="zip"):
        csvs = list_csvs_in_zip(zip_key, download_local)
        if csvs:
            zip_to_csvs[zip_key] = csvs
    total_csvs = sum(len(csvs) for csvs in zip_to_csvs.values())
    logging.info(f"Total CSVs found in {source_name}: {total_csvs}")
    if total_csvs == 0:
        logging.warning(f"No CSVs found in {source_name}")
    return zip_to_csvs

'''-------------------------------------------------------------
Comparison Functions
-------------------------------------------------------------'''
def compare_csvs(df1, df2, file_name, num_chunks=3):
    summary = {
        'Missing Columns in Neoprice': [],
        'Missing Columns in Engine': [],
        'Missing Rows in Neoprice': 0,
        'Extra Rows in Neoprice': 0,
        'Duplicate Rows in Engine': 0,
        'Duplicate Rows in Neoprice': 0,
        'Total Fields Compared': 0,
        'Number of Discrepancies': 0,
        'Number of Row Discrepancies': 0,
        'Field Mismatches': 0,
        'Failure %': 0.0,
        'Pass %': 0.0,
        'Row Failure %': 0.0,
        'Row Pass %': 0.0,
        'Status': 'PASS',
        'Total Rows in Engine': 0,
        'Total Rows in Neoprice': 0,
        'Global Percentage Metrics': {col: {'Matches': 0, 'Mismatches': 0, 'Total': 0} for col in global_percentage}
    }
    diff_summary = []

    if csv_columns:
        df1 = df1[csv_columns]
        df2 = df2[csv_columns]

    summary['Missing Columns in Neoprice'] = list(set(df1.columns) - set(df2.columns))
    summary['Missing Columns in Engine'] = list(set(df2.columns) - set(df1.columns))

    common_columns = list(set(df1.columns).intersection(df2.columns))
    if not common_columns:
        logging.info(f"No common columns to compare in {file_name}")
        return pd.DataFrame(), summary

    df1 = df1.reset_index(drop=True)
    df2 = df2.reset_index(drop=True)
    df1['_original_row'] = df1.index + 1
    df2['_original_row'] = df2.index + 1

    for key in csv_primary_keys:
        df1[key] = df1[key].astype(str).str.strip()
        df2[key] = df2[key].astype(str).str.strip()

    for col in csv_primary_keys:
        if len(df1) > 0 and df1[col].nunique() / len(df1) < 0.5:
            df1[col] = df1[col].astype('category')
            df2[col] = df2[col].astype('category')

    df1 = df1.dropna(subset=csv_primary_keys)
    df2 = df2.dropna(subset=csv_primary_keys)

    summary['Total Rows in Engine'] = len(df1)
    summary['Total Rows in Neoprice'] = len(df2)

    df1 = df1.set_index(csv_primary_keys).sort_index()
    df2 = df2.set_index(csv_primary_keys).sort_index()

    summary['Duplicate Rows in Engine'] = df1.index.duplicated().sum()
    summary['Duplicate Rows in Neoprice'] = df2.index.duplicated().sum()

    dup_rows_engine = df1[df1.index.duplicated(keep=False)]
    dup_rows_neoprice = df2[df2.index.duplicated(keep=False)]

    for pk in dup_rows_engine.index.unique():
        row_numbers = dup_rows_engine.loc[pk]['_original_row'].tolist()
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'DUPLICATE_ROW',
            'Engine_Value': f"Duplicate ({len(row_numbers)} occurrences)",
            'Neoprice_Value': '',
            'RowNum_Engine': ', '.join(map(str, row_numbers)),
            'RowNum_Neoprice': '',
            'Status': 'Duplicate in Engine'
        })

    for pk in dup_rows_neoprice.index.unique():
        row_numbers = dup_rows_neoprice.loc[pk]['_original_row'].tolist()
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'DUPLICATE_ROW',
            'Engine_Value': '',
            'Neoprice_Value': f"Duplicate ({len(row_numbers)} occurrences)",
            'RowNum_Engine': '',
            'RowNum_Neoprice': ', '.join(map(str, row_numbers)),
            'Status': 'Duplicate in Neoprice'
        })

    df1 = df1[~df1.index.duplicated()]
    df2 = df2[~df2.index.duplicated()]

    missing_in_neoprice = df1.index.difference(df2.index)
    extra_in_neoprice = df2.index.difference(df1.index)

    summary['Missing Rows in Neoprice'] = len(missing_in_neoprice)
    summary['Extra Rows in Neoprice'] = len(extra_in_neoprice)

    for pk in missing_in_neoprice:
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'MISSING_ROW',
            'Engine_Value': 'Exists',
            'Neoprice_Value': 'Missing',
            'RowNum_Engine': df1.loc[pk, '_original_row'],
            'RowNum_Neoprice': '',
            'Status': 'Missing in Neoprice'
        })

    for pk in extra_in_neoprice:
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'EXTRA_ROW',
            'Engine_Value': 'Missing',
            'Neoprice_Value': 'Exists',
            'RowNum_Engine': '',
            'RowNum_Neoprice': df2.loc[pk, '_original_row'],
            'Status': 'Extra in Neoprice'
        })

    common_idx = df1.index.intersection(df2.index)
    total_fields = 0
    mismatches = 0
    discrepant_rows = set()

    iterator = tqdm(
        common_idx,
        desc=f"Comparing rows ({file_name})",
        unit="rows",
        dynamic_ncols=True,
        leave=False,
        position=num_chunks
    ) if show_row_progress else common_idx

    for idx in iterator:
        row1 = df1.loc[idx]
        row2 = df2.loc[idx]
        row1_number = int(row1['_original_row'])
        row2_number = int(row2['_original_row'])

        row_has_mismatch = False
        for col in common_columns:
            val1 = row1.get(col, None)
            val2 = row2.get(col, None)
            total_fields += 1

            if col in global_percentage:
                summary['Global Percentage Metrics'][col]['Total'] += 1
                if values_equal(val1, val2):
                    summary['Global Percentage Metrics'][col]['Matches'] += 1
                else:
                    summary['Global Percentage Metrics'][col]['Mismatches'] += 1

            if not values_equal(val1, val2):
                mismatches += 1
                row_has_mismatch = True
                diff_summary.append({
                    'PrimaryKey': idx,
                    'Column': col,
                    'Engine_Value': val1,
                    'Neoprice_Value': val2,
                    'RowNum_Engine': row1_number,
                    'RowNum_Neoprice': row2_number,
                    'Status': 'Mismatch'
                })

        if row_has_mismatch:
            discrepant_rows.add(idx)

    discrepant_rows.update(missing_in_neoprice)
    discrepant_rows.update(extra_in_neoprice)
    discrepant_rows.update(dup_rows_engine.index)
    discrepant_rows.update(dup_rows_neoprice.index)

    summary['Total Fields Compared'] = total_fields
    summary['Field Mismatches'] = mismatches
    summary['Number of Row Discrepancies'] = len(discrepant_rows)

    missing_rows = len(missing_in_neoprice)
    extra_rows = len(extra_in_neoprice)
    duplicates = summary['Duplicate Rows in Engine'] + summary['Duplicate Rows in Neoprice']
    field_mismatches = mismatches

    total_discrepancies = missing_rows + extra_rows + duplicates + field_mismatches
    summary['Number of Discrepancies'] = total_discrepancies

    total_data_points = len(df1) + len(df2) + total_discrepancies
    if total_data_points > 0:
        failure_percent = (total_discrepancies / total_data_points) * 100
        summary['Failure %'] = round(failure_percent, 6)
        summary['Pass %'] = round(100 - summary['Failure %'], 6)
    else:
        summary['Failure %'] = 0.0
        summary['Pass %'] = 100.0

    total_engine_rows = summary['Total Rows in Engine']
    if total_engine_rows > 0:
        row_failure_percent = (summary['Number of Row Discrepancies'] / total_engine_rows) * 100
        summary['Row Failure %'] = round(row_failure_percent, 6)
        summary['Row Pass %'] = round(100 - row_failure_percent, 6)
    else:
        summary['Row Failure %'] = 0.0
        summary['Row Pass %'] = 100.0

    if total_discrepancies > 0:
        summary['Status'] = 'FAIL'
        summary['Note'] = f'‚ùå Found {total_discrepancies} discrepancies'
    else:
        summary['Status'] = 'PASS'
        summary['Note'] = '‚úÖ No comparison issues, files are identical'

    return pd.DataFrame(diff_summary), summary

def process_csv_pair(args):
    normalized_csv_name, source1_csv_map, source2_csv_map, progress_dict, chunk_id, num_chunks = args
    start_time = datetime.now()
    pid = os.getpid()
    logging.info(f"Starting {normalized_csv_name} in process {pid} at {start_time}")

    try:
        zip1, csv1_name = source1_csv_map[normalized_csv_name]
        zip2, csv2_name = source2_csv_map[normalized_csv_name]

        read_start = datetime.now()
        df1 = read_csv_from_zip(zip1, csv1_name, download_local)
        df2 = read_csv_from_zip(zip2, csv2_name, download_local)
        read_time = datetime.now() - read_start

        if df1 is None or df2 is None:
            thread_safe_print(f"‚ö†Ô∏è Skipping comparison for {normalized_csv_name} due to read error")
            with progress_dict[chunk_id]['lock']:
                progress_dict[chunk_id]['completed'] += 1
            return normalized_csv_name, pd.DataFrame(), {'Status': 'ERROR', 'Note': 'Failed to read CSV'}, chunk_id

        compare_start = datetime.now()
        diff_df, summary = compare_csvs(df1, df2, normalized_csv_name, num_chunks)
        compare_time = datetime.now() - compare_start

        del df1, df2
        gc.collect()

        end_time = datetime.now()
        logging.info(f"Finished {normalized_csv_name} in process {pid} at {end_time}, "
                     f"read={read_time}, compare={compare_time}, total={end_time - start_time}")

        with progress_dict[chunk_id]['lock']:
            progress_dict[chunk_id]['completed'] += 1
            progress_dict[chunk_id]['total_time'] += (end_time - start_time).total_seconds()

        return normalized_csv_name, diff_df, summary, chunk_id
    except Exception as e:
        thread_safe_print(f"‚ùå Error comparing {normalized_csv_name}: {e}")
        logging.error(f"Error comparing {normalized_csv_name} in process {pid}: {e}")
        with progress_dict[chunk_id]['lock']:
            progress_dict[chunk_id]['completed'] += 1
        return normalized_csv_name, pd.DataFrame(), {'Status': 'ERROR', 'Note': str(e)}, chunk_id

def process_chunk(chunk_id, chunk_csvs, source1_csv_map, source2_csv_map, progress_dict, max_workers, num_chunks):
    pid = os.getpid()
    chunk_start_time = datetime.now()
    logging.info(f"Chunk {chunk_id + 1} started | PID {pid} | CSVs {len(chunk_csvs)} | Start Time {chunk_start_time}")
    log_resource_usage()

    process_args = [
        (csv_name, source1_csv_map, source2_csv_map, progress_dict, chunk_id, num_chunks)
        for csv_name in chunk_csvs
    ]
    results = []
    with Pool(processes=max_workers) as pool:
        with tqdm(
            total=len(chunk_csvs),
            desc=f"Chunk {chunk_id + 1}",
            unit="csv",
            position=chunk_id,
            file=sys.stdout,
            dynamic_ncols=True,
            leave=True
        ) as pbar:
            for result in pool.imap_unordered(process_csv_pair, process_args):
                results.append(result)
                with tqdm_lock:
                    pbar.update(1)

    chunk_end_time = datetime.now()
    duration = (chunk_end_time - chunk_start_time).total_seconds()
    with progress_dict[chunk_id]['lock']:
        progress_dict[chunk_id]['completed'] = len(chunk_csvs)
        progress_dict[chunk_id]['total_time'] += duration
    logging.info(f"Chunk {chunk_id + 1} finished | PID {pid} | End Time {chunk_end_time} | Duration {duration:.2f}s")
    log_resource_usage()
    gc.collect()
    return results

def compare_all_csvs(source1_zip_to_csvs, source2_zip_to_csvs, use_multithreading=True, chunk_size=None):
    if not source1_zip_to_csvs or not source2_zip_to_csvs:
        logging.error("One or both ZIP-to-CSV mappings are empty")
        return pd.DataFrame(), {'Status': 'ERROR', 'Note': 'No CSVs available for comparison'}

    source1_csv_map = {
        normalize_filename(csv_name): (zip_key, csv_name)
        for zip_key, csvs in source1_zip_to_csvs.items()
        for csv_name in csvs
    }
    source2_csv_map = {
        normalize_filename(csv_name): (zip_key, csv_name)
        for zip_key, csvs in source2_zip_to_csvs.items()
        for csv_name in csvs
    }

    common_csvs = list(set(source1_csv_map) & set(source2_csv_map))
    missing_in_source2 = set(source1_csv_map) - set(source2_csv_map)
    missing_in_source1 = set(source2_csv_map) - set(source1_csv_map)

    if not common_csvs:
        logging.warning("No common CSVs found for comparison")
        all_summaries = {}
        if missing_in_source2 and include_missing_files:
            all_summaries["Missing in Source2"] = list(missing_in_source2)
        if missing_in_source1 and include_extra_files:
            all_summaries["Extra in Source2"] = list(missing_in_source1)
        return pd.DataFrame(), all_summaries

    all_diffs = []
    all_summaries = {}
    global_metrics = {col: {'Matches': 0, 'Mismatches': 0, 'Total': 0} for col in global_percentage}

    effective_processes = min(num_processes, len(common_csvs), os.cpu_count() or 4)
    num_chunks_used = min(num_chunks, len(common_csvs))
    logging.info(f"Using {num_chunks_used} chunks for {len(common_csvs)} CSVs")

    chunks = [[] for _ in range(num_chunks_used)]
    for i, csv in enumerate(common_csvs):
        chunks[i % num_chunks_used].append(csv)

    chunks = [chunk for chunk in chunks if chunk]
    num_chunks_used = len(chunks)
    workers_per_chunk = max(1, effective_processes // num_chunks_used) if num_chunks_used > 0 else 1

    for i, chunk in enumerate(chunks):
        logging.info(f"Chunk {i + 1}: {len(chunk)} CSVs - {chunk}")

    logging.info(f"Comparison Setup | Chunks {num_chunks_used} | Processes {effective_processes} | "
                 f"Workers/Chunk {workers_per_chunk} | CSVs {len(common_csvs)}")
    log_resource_usage()

    manager = Manager()
    progress_dict = manager.dict()
    for i in range(num_chunks_used):
        progress_dict[i] = manager.dict(completed=0, total=len(chunks[i]), total_time=0.0, lock=manager.Lock())

    try:
        if use_multithreading:
            with ProcessPoolExecutor(max_workers=num_chunks_used) as executor:
                futures = [
                    executor.submit(
                        process_chunk,
                        chunk_id,
                        chunk_csvs,
                        source1_csv_map,
                        source2_csv_map,
                        progress_dict,
                        workers_per_chunk,
                        num_chunks_used
                    )
                    for chunk_id, chunk_csvs in enumerate(chunks)
                ]
                logging.info(f"Submitted {len(futures)} chunk tasks | Start Time {datetime.now()}")
                all_results = []
                for future in as_completed(futures):
                    chunk_result_time = datetime.now()
                    results = future.result()
                    all_results.extend(results)
                    logging.info(f"Chunk completed | Result Time {chunk_result_time}")
        else:
            all_results = []
            for chunk_id, chunk_csvs in enumerate(chunks):
                results = process_chunk(
                    chunk_id,
                    chunk_csvs,
                    source1_csv_map,
                    source2_csv_map,
                    progress_dict,
                    workers_per_chunk,
                    num_chunks_used
                )
                all_results.extend(results)

        logging.info(f"All chunks completed | End Time {datetime.now()}")
        log_resource_usage()

        for csv_name, diff_df, summary, chunk_id in all_results:
            if diff_df.empty:
                summary['Note'] = '‚úÖ No differences'
            else:
                diff_df['File'] = csv_name
                all_diffs.append(diff_df)
            all_summaries[csv_name] = summary

            if 'Global Percentage Metrics' in summary:
                for col in global_percentage:
                    if col in summary['Global Percentage Metrics']:
                        global_metrics[col]['Matches'] += summary['Global Percentage Metrics'][col]['Matches']
                        global_metrics[col]['Mismatches'] += summary['Global Percentage Metrics'][col]['Mismatches']
                        global_metrics[col]['Total'] += summary['Global Percentage Metrics'][col]['Total']

            del diff_df, summary
            gc.collect()

        global_percentages = {}
        for col in global_percentage:
            total = global_metrics[col]['Total']
            matches = global_metrics[col]['Matches']
            mismatches = global_metrics[col]['Mismatches']
            if total > 0:
                pass_percentage = (matches / total) * 100
                global_percentages[col] = {
                    'Pass %': round(pass_percentage, 2),
                    'Fail Count': mismatches,
                    'Total Rows': total
                }
            else:
                global_percentages[col] = {
                    'Pass %': 100.0,
                    'Fail Count': 0,
                    'Total Rows': 0
                }

        all_summaries['Global Percentages'] = global_percentages

        if missing_in_source2 and include_missing_files:
            all_summaries["Missing in Source2"] = list(missing_in_source2)
        if missing_in_source1 and include_extra_files:
            all_summaries["Extra in Source2"] = list(missing_in_source1)

        final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
        return final_diff_df, all_summaries

    except Exception as e:
        logging.error(f"Critical error in compare_all_csvs: {e}")
        raise

'''-------------------------------------------------------------
Report Generation Functions
-------------------------------------------------------------'''
@lru_cache(maxsize=1000)
def format_value(val):
    if pd.isna(val) or val is None:
        return ""
    if isinstance(val, (int, float)):
        return str(int(val)) if float(val).is_integer() else "{:.4f}".format(val).rstrip('0').rstrip('.')
    return str(val)

def calculate_diff(val1, val2):
    try:
        num1, num2 = np.array([val1, val2], dtype=np.float64)
        diff = num1 - num2
        return "0" if np.abs(diff) < 1e-10 else "{:.4f}".format(diff).rstrip('0').rstrip('.')
    except (ValueError, TypeError):
        return "N/A"

def process_file(args, include_passed=True):
    csv_file, file_summary, file_diff_df_dict = args
    if csv_file in ["Missing in Source2", "Extra in Source2"] or not isinstance(file_summary, dict):
        return None, 0, 0, 0, 0, 0, 0, 0
    match_status = file_summary.get('Status', 'PASS')
    if match_status == "PASS" and not include_passed:
        return None, 0, 0, 0, 0, 0, 0, 0

    row_discrepancies = file_summary.get('Number of Row Discrepancies', 0)
    engine_rows = file_summary.get('Total Rows in Engine', 0)
    neoprice_rows = file_summary.get('Total Rowsstrained', 0)
    fields = file_summary.get('Total Fields Compared', 0)
    duplicates = file_summary.get('Duplicate Rows in Engine', 0) + file_summary.get('Duplicate Rows in Neoprice', 0)
    missing_rows = file_summary.get('Missing Rows in Neoprice', 0)
    extra_rows = file_summary.get('Extra Rows in Neoprice', 0)

    file_diff_df = pd.DataFrame(file_diff_df_dict) if file_diff_df_dict else pd.DataFrame()
    icon = "<i class='fas fa-check-circle' style='color:green;'></i>" if match_status == "PASS" else "<i class='fas fa-times-circle' style='color: red;'></i>"

    diff_groups = {}
    if not file_diff_df.empty:
        grouped = file_diff_df.groupby(['PrimaryKey', 'Status'])
        for (primary_key, status), group in grouped:
            diff_groups[(primary_key, status)] = {
                'RowNum_Engine': group['RowNum_Engine'].iloc[0],
                'RowNum_Neoprice': group['RowNum_Neoprice'].iloc[0],
                'details': group.to_dict('records')
            }

    diff_table_rows = []
    for (primary_key, status), group in diff_groups.items():
        first_detail = group['details'][0]
        diff_value = calculate_diff(first_detail['Engine_Value'], first_detail['Neoprice_Value'])
        diff_class = ("positive-diff" if diff_value != "N/A" and float(diff_value) > 0 else 
                     "negative-diff" if diff_value != "N/A" and float(diff_value) < 0 else "")
        diff_table_rows.append({
            'rowspan': len(group['details']),
            'primary_key': html.escape(str(tuple(x if x != 'nan' else '' for x in primary_key))),
            'row_num_engine': group['RowNum_Engine'] or '-',
            'row_num_neoprice': group['RowNum_Neoprice'] or '-',
            'column': html.escape(str(first_detail['Column'])),
            'engine_value': html.escape(format_value(first_detail['Engine_Value'])),
            'neoprice_value': html.escape(format_value(first_detail['Neoprice_Value'])),
            'diff_value': diff_value,
            'diff_class': diff_class,
            'status': status,
            'is_first': True
        })
        for detail in group['details'][1:]:
            diff_value = calculate_diff(detail['Engine_Value'], detail['Neoprice_Value'])
            diff_class = ("positive-diff" if diff_value != "N/A" and float(diff_value) > 0 else 
                         "negative-diff" if diff_value != "N/A" and float(diff_value) < 0 else "")
            diff_table_rows.append({
                'column': html.escape(str(detail['Column'])),
                'engine_value': html.escape(format_value(detail['Engine_Value'])),
                'neoprice_value': html.escape(format_value(detail['Neoprice_Value'])),
                'diff_value': diff_value,
                'diff_class': diff_class,
                'is_first': False
            })

    xrow_disc = row_discrepancies - (missing_rows + extra_rows + duplicates)
    mismatch_details = {
        'is_fail': match_status == "FAIL",
        'toggle_id': f"diff-{csv_file}",
        'row_discrepancies': row_discrepancies,
        'row_failure_percent': f"{file_summary.get('Row Failure %', 0.0):.6f}%" if file_summary.get('Number of Discrepancies', 0) > 0 else "",
        'xrow_disc': xrow_disc,
        'missing_rows': missing_rows,
        'extra_rows': extra_rows,
        'duplicates': duplicates,
        'diff_table_rows': diff_table_rows
    } if match_status == "FAIL" else {"message": "The files are identical. No differences were found during the comparison."}

    comparison_row = {
        'csv_file': csv_file,
        'icon': icon,
        'mismatch_details': mismatch_details
    }
    gc.collect()
    return comparison_row, row_discrepancies, engine_rows, neoprice_rows, fields, duplicates, missing_rows, extra_rows

def generate_html_report(
    diff_df,
    summary,
    report_start_time,
    output_file,
    source_files_count,
    destination_files_count,
    primary_key_columns=None,
    columns=None,
    project_name="Project",
    project_logo="logo.png",
    include_passed=True,
    include_missing_files=True,
    include_extra_files=True,
    use_multithreading=True
):
    report_end_time = datetime.now()
    time_taken = report_end_time - report_start_time
    time_taken_str = str(time_taken).split('.')[0]

    file_diff_dfs = dict(tuple(diff_df.groupby('File'))) if not diff_df.empty else {}
    file_diff_dfs = {
        csv_file: df for csv_file, df in file_diff_dfs.items()
        if csv_file not in ["Missing in Source2", "Extra in Source2"]
    }

    metrics_lock = ThreadLock()
    total_row_discrepancies = total_engine_rows = total_neoprice_rows = 0
    total_fields_compared = total_duplicates = total_missing_rows = total_extra_rows = 0

    required_columns = ['PrimaryKey', 'Status', 'RowNum_Engine', 'RowNum_Neoprice',
                       'Engine_Value', 'Neoprice_Value', 'Column']
    missing_cols = [col for col in required_columns if col not in diff_df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns in diff_df: {missing_cols}")

    comparison_rows_list = []
    process_args = [
        (csv_file, file_summary, file_diff_dfs.get(csv_file, pd.DataFrame()).to_dict('records'))
        for csv_file, file_summary in summary.items()
        if csv_file not in ['Global Percentages']
    ]
    if use_multithreading:
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = [executor.submit(process_file, args, include_passed) for args in process_args]
            for future in tqdm(futures, desc="Generating Report", unit="file"):
                result = future.result()
                if result:
                    comparison_row, row_disc, eng_rows, neo_rows, fields, dups, miss_rows, ext_rows = result
                    if comparison_row:
                        comparison_rows_list.append(comparison_row)
                    with metrics_lock:
                        total_row_discrepancies += row_disc
                        total_engine_rows += eng_rows
                        total_neoprice_rows += neo_rows
                        total_fields_compared += fields
                        total_duplicates += dups
                        total_missing_rows += miss_rows
                        total_extra_rows += ext_rows
    else:
        for args in tqdm(process_args, desc="Generating Report", unit="file"):
            result = process_file(args, include_passed)
            if result:
                comparison_row, row_disc, eng_rows, neo_rows, fields, dups, miss_rows, ext_rows = result
                if comparison_row:
                    comparison_rows_list.append(comparison_row)
                total_row_discrepancies += row_disc
                total_engine_rows += eng_rows
                total_neoprice_rows += neo_rows
                total_fields_compared += fields
                total_duplicates += dups
                total_missing_rows += miss_rows
                total_extra_rows += ext_rows

    overall_row_pass = 100.0 if total_engine_rows == 0 else round(
        ((total_engine_rows - total_row_discrepancies) / total_engine_rows) * 100, 5
    )

    global_percentages = summary.get('Global Percentages', {})
    global_metrics = [
        {
            'column': html.escape(col),
            'pass_percentage': metrics['Pass %'],
            'fail_count': metrics['Fail Count'],
            'total_rows': metrics['Total Rows'],
            'metric_class': "pass-metric" if metrics['Pass %'] >= 99.99 else "fail-metric" if metrics['Pass %'] < 90 else "warn-metric"
        }
        for col, metrics in global_percentages.items()
    ]

    missing_files = summary.get("Missing in Source2", []) if include_missing_files else []
    extra_files = summary.get("Extra in Source2", []) if include_extra_files else []

    template_content = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{{ project_name }} Report</title>
    <link rel="icon" type="image/x-icon" href="{{ project_logo }}">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        body { font-family: Arial, sans-serif; background: #f4f4f9; margin: 0; padding: 0; }
        .container { background: white; margin: 10px auto; padding: 10px; max-width: 98%; borderƒôdzi: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        header { background-color: #173E72; color: white; padding: 5px; text-align: center; }
        table { width: 100%; border-collapse: collapse; margin-top: 15px; word-break: break-word; table-layout: fixed; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background: #173E72; color: white; position: sticky; top: 0; }
        tr:nth-child(even) { background: #f9f9f9; }
        tr:hover { background: #f1f1f1; }
        .toggle-button { font-size: 0.8em; padding: 3px 8px; background-color: #0056b3; color: white; border: none; border-radius: 4px; cursor: pointer; }
        .summary-badge { font-weight: bold; }
        .diff-table th { background: #173E72; }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 10px;
            margin-top: 15px;
        }
        .summary-item {
            background: #f8f9fa;
            padding: 8px;
            border-radius: 4px;
            border-left: 4px solid #173E72;
        }
        .summary-label {
            font-weight: bold;
            color: #495057;
        }
        .summary-value {
            color: #212529;
            margin-left: 5px;
        }
        .numeric-diff {
            font-family: monospace;
            text-align: right;
        }
        .positive-diff { color: #d9534f; }
        .negative-diff { color: #5cb85c; }
        header img {
            height: 60px;
            width: auto;
            max-width: 200px;
        }
        .metrics-container {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .metric-card {
            background: white;
            border-radius: 8px;
            padding: 15px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            text-align: center;
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            margin: 5px 0;
        }
        .metric-label {
            color: #6c757d;
            font-size: 14px;
        }
        .metric-subtext {
            font-size: 12px;
            color: #6c757d;
            margin-top: 5px;
        }
        .smaller-text {
            font-size: 0.9em;
        }
        .pass-metric { border-top: 4px solid #28a745; }
        .fail-metric { border-top: 4px solid #dc3545; }
        .warn-metric { border-top: 4px solid #ffc107; }
        .neutral-metric { border-top: 4px solid #66b2ff; }
    </style>
    <script>
        function toggleVisibility(id, btn) {
            var el = document.getElementById(id);
            if (el.style.display === 'none') {
                el.style.display = 'block';
                btn.innerHTML = '-';
            } else {
                el.style.display = 'none';
                btn.innerHTML = '+';
            }
        }
    </script>
</head>
<body>
    <header>
    <table style="border: none; width: 100%;">
        <tr>
            <td style="border: none; text-align: left; width: 20%; padding: 5px 0;">
                <img src="{{ project_logo }}" alt="{{ project_name }}" style="height: 40px; width: auto;">
            </td>
            <td style="border: none; text-align: center; padding: 5px 0;">
                <h1 style="margin: 0; font-size: 1.5em;">{{ project_name }} - CSV Comparison Report</h1>
                <p style="margin: 2px 0 0; font-size: 0.9em;"> <strong>Generated:</strong> {{ report_start_time }} | <strong>Duration:</strong> {{ time_taken_str }}</p>
            </td>
            <td style="border: none; text-align: right; width: 20%; padding: 5px 0;">
                <div style="font-size: 1.2em; font-weight: bold; color: {% if overall_row_pass >= 99.99 %}#28a745{% else %}#dc3545{% endif %}">
                    {{ overall_row_pass }}% Row Pass
                </div>
            </td>
        </tr>
    </table>
</header>
<div class="container">
    <h2>üìä Key Metrics</h2>
    <div class="metrics-container">
        <div class="metric-card pass-metric">
            <div class="metric-value">{{ overall_row_pass }}%</div>
            <div class="metric-label">Overall Row Pass Rate</div>
        </div>
        <div class="metric-card fail-metric">
            <div class="metric-value">{{ total_row_discrepancies }}</div>
            <div class="metric-label">Row Discrepancies</div>
        </div>
        <div class="metric-card neutral-metric">
            <div class="metric-value">{{ total_engine_rows }}</div>
            <div class="metric-label">Total Engine Rows</div>
        </div>
        <div class="metric-card neutral-metric">
            <div class="metric-value">{{ total_neoprice_rows }}</div>
            <div class="metric-label">Total Neoprice Rows</div>
        </div>
        <div class="metric-card warn-metric">
            <div class="metric-value">{{ total_missing_rows }}</div>
            <div class="metric-label">Missing Rows</div>
        </div>
        <div class="metric-card warn-metric">
            <div class="metric-value">{{ total_extra_rows }}</div>
            <div class="metric-label">Extra Rows</div>
        </div>
        <div class="metric-card warn-metric">
            <div class="metric-value">{{ total_duplicates }}</div>
            <div class="metric-label">Duplicate Rows</div>
        </div>
    </div>
    <h2>üìà Global Column Pass Percentages</h2>
    <div class="metrics-container">
        {% for metric in global_metrics %}
            <div class="metric-card {{ metric.metric_class }}">
                <div class="metric-value">{{ metric.pass_percentage }}%</div>
                <div class="metric-label">{{ metric.column }}</div>
                <div class="metric-subtext">{{ metric.fail_count }} fail out of {{ metric.total_rows }} rows</div>
            </div>
        {% endfor %}
    </div>
    <h2>üîç Comparison Details</h2>
    <ul>
        <li><strong><i class="fas fa-file-alt"></i> Files in Engine:</strong> {{ source_files_count }}</li>
        <li><strong><i class="fas fa-file-alt"></i> Files in Neoprice:</strong> {{ destination_files_count }}</li>
        <li>
            <strong><i class="fas fa-key"></i> Primary Keys:</strong> 
            <button class="toggle-button" onclick="toggleVisibility('primaryKeys', this)">+</button>
            <span id="primaryKeys" style="display:none;">
                <span class="smaller-text">{{ primary_key_columns|join(', ') }}</span>
            </span>
        </li>
        <li>
            <strong><i class="fas fa-columns"></i> Compared Columns:</strong>
            <button class="toggle-button" onclick="toggleVisibility('comparedColumns', this)">+</button>
            <span id="comparedColumns" style="display:none;">
                <span class="smaller-text">{% if not columns %}All Columns{% else %}{{ columns|join(', ') }}{% endif %}</span>
            </span>
        </li>
    </ul>
    <h2>üìù File Comparison Results</h2>
    <table>
        <thead>
            <tr>
                <th width="20%" nowrap>CSV File</th>
                <th width="5%">Status</th>
                <th>Details</th>
            </tr>
        </thead>
        <tbody>
            {% for row in comparison_rows %}
                <tr>
                    <td><i class="fas fa-file-csv" style="color:blue;"></i> {{ row.csv_file }}</td>
                    <td align="center">{{ row.icon|safe }}</td>
                    <td>
                        {% if row.mismatch_details.is_fail %}
                            <div>
                                <button class="toggle-button" onclick="toggleVisibility('{{ row.mismatch_details.toggle_id }}', this)">+</button>
                                <span class="summary-badge" style="background-color: #f8d7da; color: #721c24; padding: 2px 6px; border-radius: 4px; margin-left: 5px;">
                                    {{ row.mismatch_details.row_discrepancies }} discrepancies
                                    {% if row.mismatch_details.row_failure_percent %} | {{ row.mismatch_details.row_failure_percent }} failure{% endif %}
                                    {% if row.mismatch_details.xrow_disc > 0 %} | row discrepancies:{{ row.mismatch_details.xrow_disc }}{% endif %}
                                    {% if row.mismatch_details.missing_rows > 0 %} | missing rows:{{ row.mismatch_details.missing_rows }}{% endif %}
                                    {% if row.mismatch_details.extra_rows > 0 %} | extra rows:{{ row.mismatch_details.extra_rows }}{% endif %}
                                    {% if row.mismatch_details.duplicates > 0 %} | duplicate rows:{{ row.mismatch_details.duplicates }}{% endif %}
                                </span>
                                <div id="{{ row.mismatch_details.toggle_id }}" style="display:none; margin-top: 10px;">
                                    <table class="diff-table">
                                        <thead>
                                            <tr>
                                                <th width="50%">Primary Key</th>
                                                <th width="10%">Column</th>
                                                <th width="10%">Engine</th>
                                                <th width="10%">Neoprice</th>
                                                <th width="10%">Diff</th>
                                                <th width="10%">Status</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            {% for diff_row in row.mismatch_details.diff_table_rows %}
                                                <tr>
                                                    {% if diff_row.is_first %}
                                                        <td rowspan="{{ diff_row.rowspan }}" style="vertical-align: top;">
                                                            <small>{{ diff_row.primary_key }}</small><br>
                                                            <small>Engine Row: {{ diff_row.row_num_engine }} Neoprice Row: {{ diff_row.row_num_neoprice }}</small>
                                                        </td>
                                                    {% endif %}
                                                    <td><small>{{ diff_row.column }}</small></td>
                                                    <td><small>{{ diff_row.engine_value }}</small></td>
                                                    <td><small>{{ diff_row.neoprice_value }}</small></td>
                                                    <td class="numeric-diff {{ diff_row.diff_class }}"><small>{{ diff_row.diff_value }}</small></td>
                                                    {% if diff_row.is_first %}
                                                        <td rowspan="{{ diff_row.rowspan }}" style="vertical-align: middle;"><small>{{ diff_row.status }}</small></td>
                                                    {% endif %}
                                                </tr>
                                            {% endfor %}
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        {% else %}
                            <div style='color: green;'>{{ row.mismatch_details.message }}</div>
                        {% endif %}
                    </td>
                </tr>
            {% endfor %}
            {% for missing_csv in missing_files %}
                <tr>
                    <td><i class="fas fa-file-csv" style="color:gray;"></i> {{ missing_csv }}</td>
                    <td align="center"><i class="fas fa-exclamation-triangle" style="color:orange;"></i></td>
                    <td>Missing in Neoprice</td>
                </tr>
            {% endfor %}
            {% for extra_csv in extra_files %}
                <tr>
                    <td><i class="fas fa-file-csv" style="color:gray;"></i> {{ extra_csv }}</td>
                    <td align="center"><i class="fas fa-exclamation-triangle" style="color:blue;"></i></td>
                    <td>Extra in Neoprice</td>
                </tr>
            {% endfor %}
        </tbody>
    </table>
</div>
</body>
</html>
    """

    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            template = Template(template_content)
            chunk_size = 100
            for i in range(0, len(comparison_rows_list), chunk_size):
                chunk_rows = comparison_rows_list[i:i + chunk_size]
                html_chunk = template.render(
                    project_name=project_name,
                    project_logo=project_logo,
                    report_start_time=report_start_time.strftime('%Y-%m-%d %H:%M:%S'),
                    time_taken_str=time_taken_str,
                    overall_row_pass=overall_row_pass,
                    total_row_discrepancies=total_row_discrepancies,
                    total_engine_rows=total_engine_rows,
                    total_neoprice_rows=total_neoprice_rows,
                    total_missing_rows=total_missing_rows,
                    total_extra_rows=total_extra_rows,
                    total_duplicates=total_duplicates,
                    global_metrics=global_metrics,
                    source_files_count=source_files_count,
                    destination_files_count=destination_files_count,
                    primary_key_columns=primary_key_columns or [],
                    columns=columns,
                    comparison_rows=chunk_rows,
                    missing_files=missing_files if i == 0 else [],
                    extra_files=extra_files if i == 0 else []
                )
                f.write(html_chunk)
                f.flush()
                gc.collect()
    except Exception as e:
        logging.error(f"Failed to generate HTML report: {e}")
        raise

'''-------------------------------------------------------------
Main Execution Functions
-------------------------------------------------------------'''
def run_comparison(download_local=True):
    try:
        source1_zips = list_zip_files(source_1_prefix, download_local)
        source2_zips = list_zip_files(source_2_prefix, download_local)

        if not source1_zips or not source2_zips:
            logging.error("No ZIP files found for comparison")
            return pd.DataFrame(), {'Status': 'ERROR', 'Note': 'No ZIP files available'}, [0, 0]

        source1_zip_to_csvs = read_all_csvs_by_source(source1_zips, "source1", download_local)
        source2_zip_to_csvs = read_all_csvs_by_source(source2_zips, "source2", download_local)

        logging.info(f"source1_zip_to_csvs: {len(source1_zip_to_csvs)} ZIPs, "
                    f"{sum(len(csvs) for csvs in source1_zip_to_csvs.values())} CSVs")
        logging.info(f"source2_zip_to_csvs: {len(source2_zip_to_csvs)} ZIPs, "
                    f"{sum(len(csvs) for csvs in source2_zip_to_csvs.values())} CSVs")

        diff_df, summary = compare_all_csvs(
            source1_zip_to_csvs,
            source2_zip_to_csvs,
            use_multithreading_comparison,
            chunk_size=None
        )
        list_files = [
            sum(len(csvs) for csvs in source1_zip_to_csvs.values()),
            sum(len(csvs) for csvs in source2_zip_to_csvs.values())
        ]

        return diff_df, summary, list_files

    except Exception as e:
        logging.error(f"Error in run_comparison: {e}")
        raise

if __name__ == "__main__":
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name, ext = os.path.splitext(output_file)
        extension = ext if ext else ".html"
        output_file = f"{base_name}_{timestamp}{extension}"
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, output_file)

        logging.info('---------------- CSV Comparison Started ----------------')
        start_time = datetime.now()
        diff_df, summary, list_files = run_comparison(download_local=download_local)
        
        generate_html_report(
            diff_df=diff_df,
            summary=summary,
            report_start_time=start_time,
            output_file=output_file,
            source_files_count=list_files[0],
            destination_files_count=list_files[1],
            primary_key_columns=csv_primary_keys,
            columns=csv_columns,
            project_name=project_name,
            project_logo=project_logo,
            include_passed=include_passed,
            include_missing_files=include_missing_files,
            include_extra_files=include_extra_files,
            use_multithreading=True
        )

        logging.info('---------------- CSV Comparison Finished ----------------')
        
    except Exception as e:
        logging.error(f"Error in main execution: {e}")
        raise


num_processes = 32
comparison_batch_size = 100
num_chunks = 3
show_row_progress = True
